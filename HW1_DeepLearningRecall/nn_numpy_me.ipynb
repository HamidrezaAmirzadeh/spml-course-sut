{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ooJtNr5dGrH9"
      },
      "source": [
        "**Name:** Hamidreza Amirzadeh\n",
        "\n",
        "**Student Number:** 401206999\n",
        "\n",
        "**Note:** This work was done under collabration with Mr.Abdollahi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJm9Z1k0cdmh"
      },
      "source": [
        "# Neural-Network with Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDN075MYGesD"
      },
      "source": [
        "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3FdxgNcdmm"
      },
      "source": [
        "## Imports & Seeding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZ4zlnxqhl5"
      },
      "source": [
        "Importing some common libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Et7OS7TGcdmn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(123)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2v2-xbcdmo"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKWqV2Gycdmp"
      },
      "source": [
        "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMYZtSoLc7c-",
        "outputId": "c6134354-e7f9-41ad-84a4-bb4af2e29b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(70000, 784) (70000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "\n",
        "\n",
        "# Using `fetch_openml`, download `Fashion-MNIST` \n",
        "# and save the training data and labels in `X` and `y` respectively.\n",
        "#############################\n",
        "# Your code goes here (5 points)\n",
        "fmnist_dataset = fetch_openml(name=\"Fashion-MNIST\", version=1)\n",
        "data = fmnist_dataset.data\n",
        "target = fmnist_dataset.target\n",
        "X = data.to_numpy()\n",
        "y = target.to_numpy()\n",
        "#############################\n",
        "\n",
        "# Normalization:\n",
        "X = ((X / 255.) - .5) * 2\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDmxyMJ4dBk3",
        "outputId": "cb1fb477-8772-478a-c1f8-89a3c81d2aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(52500, 784) (52500,) (17500, 784) (17500,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using `train_test_split`, split your data into two sets. \n",
        "# Set the test_size to 10000\n",
        "\n",
        "#############################\n",
        "# Your code goes here (6 points)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiGTXGXKcdmt"
      },
      "source": [
        "## Prepare training & validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3nNYlDcdmt"
      },
      "source": [
        "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
        "\n",
        "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
        "\n",
        "In this part, you'll limit the testing and training sets to only these three classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcBDZEtzcdmu",
        "outputId": "fb6871d8-4a18-45fa-c68d-61f8576e0c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15697, 784) (15697,)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_train` and `x_train`.\n",
        "# Only keep the 3 classes mentioned above. \n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "bool_0 = y_train=='0'\n",
        "bool_1 = y_train=='1'\n",
        "bool_7 = y_train=='7'\n",
        "bool_all = bool_0 | bool_1 | bool_7\n",
        "\n",
        "x_train = x_train[bool_all, :]\n",
        "y_train = y_train[bool_all]\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LX2hkRe1cdmw"
      },
      "outputs": [],
      "source": [
        "# Modify `y_test` and `x_test`.\n",
        "# Only keep the 3 classes mentioned above. \n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "bool_0 = y_test=='0'\n",
        "bool_1 = y_test=='1'\n",
        "bool_7 = y_test=='7'\n",
        "bool_all = bool_0 | bool_1 | bool_7\n",
        "\n",
        "x_test = x_test[bool_all, :]\n",
        "y_test = y_test[bool_all]\n",
        "#############################\n",
        "\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAW8-xG-iADR"
      },
      "outputs": [],
      "source": [
        "y_train[y_train =='0'] = 0\n",
        "y_train[y_train =='1'] = 1\n",
        "y_train[y_train =='7'] = 2\n",
        "\n",
        "y_test[y_test =='0'] = 0\n",
        "y_test[y_test =='1'] = 1\n",
        "y_test[y_test =='7'] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv6SMLUktWbv"
      },
      "source": [
        "## Linear & Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlyJo5JteKC"
      },
      "source": [
        "In this part, you'll implement the forward and backward process for the following components:\n",
        "- Softmax Layer\n",
        "- Linear Layer\n",
        "- ReLU Layer\n",
        "- Sigmoid Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXtAD5uYA4sQ"
      },
      "source": [
        "### The `Softmax` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tzaIVo-_Axp7"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLayer(object):\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Write the forward pass for softmax.\n",
        "        # Save the values required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        assert len(x.shape) == 2\n",
        "        s = np.max(x, axis=1)\n",
        "        s = s[:, np.newaxis]\n",
        "        e_x = np.exp(x - s)\n",
        "        div = np.sum(e_x, axis=1)\n",
        "        div = div[:, np.newaxis]\n",
        "        self.output = e_x / div\n",
        "        return self.output\n",
        "        #############################\n",
        "\n",
        "    def backward(self, up_grad,y):\n",
        "        # Write the backward pass for softmax.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        for i in range(len(self.output)):\n",
        "          self.output[i][np.argmax(y[i])]-=1\n",
        "        return self.output\n",
        "        #############################\n",
        "\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcFoIDZjcdnB"
      },
      "source": [
        "### The `Linear` Layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1strsTh6cdnG"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        # Initialize the layer's weights and biases\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "        self.weights = (np.random.rand(in_dim, out_dim)-0.5) * 0.2\n",
        "        self.bias = (np.random.rand(1, out_dim) - 0.5 ) * 0.2        \n",
        "        #############################\n",
        "        self.dw = None\n",
        "        self.db = None\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        # Compute linear layer's output.\n",
        "        # Save the value(s) required for the backward phase.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        z = np.dot(inp , self.weights) + self.bias\n",
        "        #############################\n",
        "        \n",
        "        return z\n",
        "    \n",
        "    def backward(self, input ,up_grad):\n",
        "        # Calculate the gradient with respect to the weights \n",
        "        # and biases and save the results.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        down_grad = np.dot(up_grad, self.weights.T)\n",
        "        batchSize = input.shape[1]\n",
        "        biasMatrixShape = (self.bias.shape[0], self.bias.shape[1])\n",
        "        updateOfWeights = np.dot(input.T, up_grad)/batchSize\n",
        "        updateOfbias =  np.mean(up_grad, axis=0).reshape(biasMatrixShape)\n",
        "        self.dw = updateOfWeights\n",
        "        self.db = updateOfbias\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      # Update the layer's weights and biases\n",
        "      # Update previous_w_update and previous_b_update accordingly\n",
        "      #############################\n",
        "      # Your code goes here (5 points)\n",
        "      self.weights = optimizer.get_next_update(self.weights ,self.dw )\n",
        "      self.bias = optimizer.get_next_update(self.bias ,self.db )\n",
        "      #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Lfo-nhcdnG"
      },
      "source": [
        "### The `ReLU` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tN6vcirMcdnH"
      },
      "outputs": [],
      "source": [
        "class RelU:\n",
        "    def __init__(self ):\n",
        "        self.inp = None\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Write the forward pass for ReLU.\n",
        "        # Save the value(s) required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = inp\n",
        "        output =  self.inp * (self.inp > 0)\n",
        "        #############################\n",
        "        return output\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        def reluDerivative(x):\n",
        "            x[x<=0] = 0\n",
        "            x[x>0] = 1\n",
        "            return x\n",
        "        down_grad =  reluDerivative(self.inp) * up_grad\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00KoSI3cdnJ"
      },
      "source": [
        "### The `sigmoid` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TTYYeL2lcdnJ"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, inp):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = inp\n",
        "        self.out =  1.0 / (1 + np.exp(-self.inp))\n",
        "        #############################\n",
        "        return self.out\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        down_grad =  (1.0 / (1 + np.exp(-self.inp))) * (1 - (1.0 / (1 + np.exp(-self.inp)))) * up_grad\n",
        "        #############################\n",
        "        return down_grad\n",
        "    \n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zngleGY2cdnK"
      },
      "source": [
        "## `Loss` function :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISedT4FvcdnK"
      },
      "source": [
        "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XQyz4ybycdnL"
      },
      "outputs": [],
      "source": [
        "class CELoss():\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \n",
        "        self.yhat = pred\n",
        "        self.y = target\n",
        "\n",
        "        # Commpute and return the loss \n",
        "        #############################\n",
        "        # Your code goes here (8 points)\n",
        "        lossData = []\n",
        "        for i in range(len(self.yhat)):\n",
        "          output = self.yhat[i][np.argmax(self.y[i])]\n",
        "          lossData.append(-np.log(output))\n",
        "        loss = np.mean(lossData)\n",
        "        return loss\n",
        "        #############################\n",
        "        \n",
        "    \n",
        "    def backward(self):\n",
        "        # Derivative of loss_fn with respect to a the predicted label.\n",
        "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        grad = []\n",
        "        for i in range(len(self.yhat)):\n",
        "            grad.append(-(1/self.yhat[i][np.argmax(self.y[i])]))\n",
        "        grad = np.array(grad)\n",
        "        #############################\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xovZI-70kB9I"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "In this section, you'll implement an optimizer classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h5ADTi5tkVTS"
      },
      "outputs": [],
      "source": [
        "class GradientDescent(object):\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def get_next_update(self, x, dx):\n",
        "        # Compute the new value for 'x' and return the result\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "        return x - (self.lr * dx)\n",
        "        #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxrEEovYEFi"
      },
      "source": [
        "## The Model\n",
        "Now you'll write the base class for a multi-layer perceptron network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "t8SoZeYRcdnY"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers, loss_fn, optimizer):\n",
        "      \n",
        "        self.layers = layers \n",
        "        self.losses  = [] \n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.activations = []\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Pass `inp` to all the layers sequentially\n",
        "        # and return the result.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.activations = [inp]\n",
        "        for i in range(len(self.layers)):\n",
        "          self.activations.append(self.layers[i].forward(self.activations[-1]))\n",
        "        return self.activations\n",
        "        #############################\n",
        "        \n",
        "    def loss(self, pred, label):\n",
        "        loss = self.loss_fn.forward(pred, label)\n",
        "        self.losses.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        Y = self.forward(X)[-1]\n",
        "        return Y.argmax(axis=-1)\n",
        "\n",
        "    def backward(self , y):\n",
        "        # Start with loss function's gradient and \n",
        "        # do the backward pass on all the layers.\n",
        "        #############################\n",
        "        # Your code goes here (5 points)\n",
        "        loss = self.loss(self.activations[-1], y) \n",
        "        loss_grad = self.loss_fn.backward()\n",
        "        loss_grad = self.layers[5].backward(loss_grad , y)\n",
        "        loss_grad = self.layers[4].backward( self.activations[-3], loss_grad)\n",
        "        loss_grad = self.layers[3].backward(loss_grad)\n",
        "        loss_grad = self.layers[2].backward( self.activations[-5] , loss_grad)\n",
        "        loss_grad = self.layers[1].backward(loss_grad)\n",
        "        loss_grad = self.layers[0].backward( self.activations[-7] , loss_grad)\n",
        "        return np.mean(loss)\n",
        "        #############################\n",
        "        \n",
        "    def update(self):\n",
        "        for layer in self.layers:\n",
        "          layer.step(self.optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0rNwYciueF"
      },
      "source": [
        "The following cell encodes training labels into a one-hot representation with 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nhJTulaFJ4vR"
      },
      "outputs": [],
      "source": [
        "def onehot_enc(y, num_labels):\n",
        "    ary = np.zeros((y.shape[0], num_labels))\n",
        "    for i, val in enumerate(y):\n",
        "        ary[i, val] = 1\n",
        "    return ary\n",
        "y_train = onehot_enc(y_train, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TS6S_RUwsRkF"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, x, y):\n",
        "    for n in range(epochs):\n",
        "      # First do the forward pass. Next, compute the loss.\n",
        "      # Then do the backward pass and finally, update the parameters.\n",
        "      #############################\n",
        "      # Your code goes here (4 points)\n",
        "      model.forward(x)\n",
        "      loss = model.backward(y)\n",
        "      model.update()\n",
        "      #############################\n",
        "      print(f\"Loss at {n}: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1lSq2jNcdnY",
        "outputId": "73daad0d-225b-4bfd-ede1-dddc328b1e68",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at 0: 1.107\n",
            "Loss at 1: 1.888\n",
            "Loss at 2: 8.266\n",
            "Loss at 3: 1.074\n",
            "Loss at 4: 1.070\n",
            "Loss at 5: 1.064\n",
            "Loss at 6: 1.056\n",
            "Loss at 7: 1.045\n",
            "Loss at 8: 1.031\n",
            "Loss at 9: 1.013\n",
            "Loss at 10: 0.993\n",
            "Loss at 11: 0.975\n",
            "Loss at 12: 0.960\n",
            "Loss at 13: 0.949\n",
            "Loss at 14: 0.940\n",
            "Loss at 15: 0.932\n",
            "Loss at 16: 0.925\n",
            "Loss at 17: 0.919\n",
            "Loss at 18: 0.914\n",
            "Loss at 19: 0.908\n",
            "Loss at 20: 0.904\n",
            "Loss at 21: 0.899\n",
            "Loss at 22: 0.895\n",
            "Loss at 23: 0.890\n",
            "Loss at 24: 0.886\n",
            "Loss at 25: 0.882\n",
            "Loss at 26: 0.878\n",
            "Loss at 27: 0.875\n",
            "Loss at 28: 0.871\n",
            "Loss at 29: 0.867\n",
            "Loss at 30: 0.863\n",
            "Loss at 31: 0.860\n",
            "Loss at 32: 0.856\n",
            "Loss at 33: 0.853\n",
            "Loss at 34: 0.849\n",
            "Loss at 35: 0.846\n",
            "Loss at 36: 0.843\n",
            "Loss at 37: 0.839\n",
            "Loss at 38: 0.836\n",
            "Loss at 39: 0.833\n",
            "Loss at 40: 0.830\n",
            "Loss at 41: 0.826\n",
            "Loss at 42: 0.823\n",
            "Loss at 43: 0.820\n",
            "Loss at 44: 0.817\n",
            "Loss at 45: 0.815\n",
            "Loss at 46: 0.812\n",
            "Loss at 47: 0.809\n",
            "Loss at 48: 0.806\n",
            "Loss at 49: 0.804\n",
            "Loss at 50: 0.801\n",
            "Loss at 51: 0.799\n",
            "Loss at 52: 0.797\n",
            "Loss at 53: 0.794\n",
            "Loss at 54: 0.792\n",
            "Loss at 55: 0.790\n",
            "Loss at 56: 0.788\n",
            "Loss at 57: 0.786\n",
            "Loss at 58: 0.784\n",
            "Loss at 59: 0.782\n",
            "Loss at 60: 0.780\n",
            "Loss at 61: 0.778\n",
            "Loss at 62: 0.777\n",
            "Loss at 63: 0.775\n",
            "Loss at 64: 0.773\n",
            "Loss at 65: 0.771\n",
            "Loss at 66: 0.770\n",
            "Loss at 67: 0.768\n",
            "Loss at 68: 0.767\n",
            "Loss at 69: 0.765\n",
            "Loss at 70: 0.764\n",
            "Loss at 71: 0.762\n",
            "Loss at 72: 0.761\n",
            "Loss at 73: 0.760\n",
            "Loss at 74: 0.758\n",
            "Loss at 75: 0.757\n",
            "Loss at 76: 0.756\n",
            "Loss at 77: 0.754\n",
            "Loss at 78: 0.753\n",
            "Loss at 79: 0.752\n",
            "Loss at 80: 0.750\n",
            "Loss at 81: 0.749\n",
            "Loss at 82: 0.748\n",
            "Loss at 83: 0.747\n",
            "Loss at 84: 0.746\n",
            "Loss at 85: 0.744\n",
            "Loss at 86: 0.743\n",
            "Loss at 87: 0.742\n",
            "Loss at 88: 0.741\n",
            "Loss at 89: 0.740\n",
            "Loss at 90: 0.739\n",
            "Loss at 91: 0.738\n",
            "Loss at 92: 0.736\n",
            "Loss at 93: 0.735\n",
            "Loss at 94: 0.734\n",
            "Loss at 95: 0.733\n",
            "Loss at 96: 0.732\n",
            "Loss at 97: 0.731\n",
            "Loss at 98: 0.730\n",
            "Loss at 99: 0.728\n",
            "Loss at 100: 0.727\n",
            "Loss at 101: 0.726\n",
            "Loss at 102: 0.724\n",
            "Loss at 103: 0.723\n",
            "Loss at 104: 0.721\n",
            "Loss at 105: 0.719\n",
            "Loss at 106: 0.717\n",
            "Loss at 107: 0.714\n",
            "Loss at 108: 0.710\n",
            "Loss at 109: 0.704\n",
            "Loss at 110: 0.694\n",
            "Loss at 111: 0.676\n",
            "Loss at 112: 0.646\n",
            "Loss at 113: 0.604\n",
            "Loss at 114: 0.562\n",
            "Loss at 115: 0.529\n",
            "Loss at 116: 0.507\n",
            "Loss at 117: 0.490\n",
            "Loss at 118: 0.477\n",
            "Loss at 119: 0.466\n",
            "Loss at 120: 0.457\n",
            "Loss at 121: 0.450\n",
            "Loss at 122: 0.445\n",
            "Loss at 123: 0.440\n",
            "Loss at 124: 0.435\n",
            "Loss at 125: 0.431\n",
            "Loss at 126: 0.427\n",
            "Loss at 127: 0.424\n",
            "Loss at 128: 0.421\n",
            "Loss at 129: 0.418\n",
            "Loss at 130: 0.415\n",
            "Loss at 131: 0.413\n",
            "Loss at 132: 0.411\n",
            "Loss at 133: 0.408\n",
            "Loss at 134: 0.406\n",
            "Loss at 135: 0.404\n",
            "Loss at 136: 0.402\n",
            "Loss at 137: 0.400\n",
            "Loss at 138: 0.399\n",
            "Loss at 139: 0.397\n",
            "Loss at 140: 0.395\n",
            "Loss at 141: 0.393\n",
            "Loss at 142: 0.392\n",
            "Loss at 143: 0.390\n",
            "Loss at 144: 0.389\n",
            "Loss at 145: 0.387\n",
            "Loss at 146: 0.386\n",
            "Loss at 147: 0.385\n",
            "Loss at 148: 0.383\n",
            "Loss at 149: 0.382\n",
            "Loss at 150: 0.381\n",
            "Loss at 151: 0.380\n",
            "Loss at 152: 0.378\n",
            "Loss at 153: 0.377\n",
            "Loss at 154: 0.376\n",
            "Loss at 155: 0.375\n",
            "Loss at 156: 0.374\n",
            "Loss at 157: 0.373\n",
            "Loss at 158: 0.371\n",
            "Loss at 159: 0.370\n",
            "Loss at 160: 0.369\n",
            "Loss at 161: 0.368\n",
            "Loss at 162: 0.367\n",
            "Loss at 163: 0.366\n",
            "Loss at 164: 0.365\n",
            "Loss at 165: 0.364\n",
            "Loss at 166: 0.363\n",
            "Loss at 167: 0.362\n",
            "Loss at 168: 0.361\n",
            "Loss at 169: 0.360\n",
            "Loss at 170: 0.359\n",
            "Loss at 171: 0.358\n",
            "Loss at 172: 0.358\n",
            "Loss at 173: 0.357\n",
            "Loss at 174: 0.356\n",
            "Loss at 175: 0.355\n",
            "Loss at 176: 0.354\n",
            "Loss at 177: 0.353\n",
            "Loss at 178: 0.352\n",
            "Loss at 179: 0.351\n",
            "Loss at 180: 0.350\n",
            "Loss at 181: 0.350\n",
            "Loss at 182: 0.349\n",
            "Loss at 183: 0.348\n",
            "Loss at 184: 0.347\n",
            "Loss at 185: 0.346\n",
            "Loss at 186: 0.345\n",
            "Loss at 187: 0.345\n",
            "Loss at 188: 0.344\n",
            "Loss at 189: 0.343\n",
            "Loss at 190: 0.342\n",
            "Loss at 191: 0.341\n",
            "Loss at 192: 0.341\n",
            "Loss at 193: 0.340\n",
            "Loss at 194: 0.339\n",
            "Loss at 195: 0.338\n",
            "Loss at 196: 0.338\n",
            "Loss at 197: 0.337\n",
            "Loss at 198: 0.336\n",
            "Loss at 199: 0.335\n",
            "Loss at 200: 0.335\n",
            "Loss at 201: 0.334\n",
            "Loss at 202: 0.333\n",
            "Loss at 203: 0.332\n",
            "Loss at 204: 0.332\n",
            "Loss at 205: 0.331\n",
            "Loss at 206: 0.330\n",
            "Loss at 207: 0.329\n",
            "Loss at 208: 0.329\n",
            "Loss at 209: 0.328\n",
            "Loss at 210: 0.327\n",
            "Loss at 211: 0.327\n",
            "Loss at 212: 0.326\n",
            "Loss at 213: 0.325\n",
            "Loss at 214: 0.324\n",
            "Loss at 215: 0.324\n",
            "Loss at 216: 0.323\n",
            "Loss at 217: 0.322\n",
            "Loss at 218: 0.322\n",
            "Loss at 219: 0.321\n",
            "Loss at 220: 0.320\n",
            "Loss at 221: 0.320\n",
            "Loss at 222: 0.319\n",
            "Loss at 223: 0.318\n",
            "Loss at 224: 0.318\n",
            "Loss at 225: 0.317\n",
            "Loss at 226: 0.316\n",
            "Loss at 227: 0.316\n",
            "Loss at 228: 0.315\n",
            "Loss at 229: 0.314\n",
            "Loss at 230: 0.314\n",
            "Loss at 231: 0.313\n",
            "Loss at 232: 0.312\n",
            "Loss at 233: 0.312\n",
            "Loss at 234: 0.311\n",
            "Loss at 235: 0.311\n",
            "Loss at 236: 0.310\n",
            "Loss at 237: 0.309\n",
            "Loss at 238: 0.309\n",
            "Loss at 239: 0.308\n",
            "Loss at 240: 0.307\n",
            "Loss at 241: 0.307\n",
            "Loss at 242: 0.306\n",
            "Loss at 243: 0.306\n",
            "Loss at 244: 0.305\n",
            "Loss at 245: 0.304\n",
            "Loss at 246: 0.304\n",
            "Loss at 247: 0.303\n",
            "Loss at 248: 0.302\n",
            "Loss at 249: 0.302\n",
            "Loss at 250: 0.301\n",
            "Loss at 251: 0.301\n",
            "Loss at 252: 0.300\n",
            "Loss at 253: 0.299\n",
            "Loss at 254: 0.299\n",
            "Loss at 255: 0.298\n",
            "Loss at 256: 0.298\n",
            "Loss at 257: 0.297\n",
            "Loss at 258: 0.297\n",
            "Loss at 259: 0.296\n",
            "Loss at 260: 0.295\n",
            "Loss at 261: 0.295\n",
            "Loss at 262: 0.294\n",
            "Loss at 263: 0.294\n",
            "Loss at 264: 0.293\n",
            "Loss at 265: 0.293\n",
            "Loss at 266: 0.292\n",
            "Loss at 267: 0.291\n",
            "Loss at 268: 0.291\n",
            "Loss at 269: 0.290\n",
            "Loss at 270: 0.290\n",
            "Loss at 271: 0.289\n",
            "Loss at 272: 0.289\n",
            "Loss at 273: 0.288\n",
            "Loss at 274: 0.287\n",
            "Loss at 275: 0.287\n",
            "Loss at 276: 0.286\n",
            "Loss at 277: 0.286\n",
            "Loss at 278: 0.285\n",
            "Loss at 279: 0.285\n",
            "Loss at 280: 0.284\n",
            "Loss at 281: 0.284\n",
            "Loss at 282: 0.283\n",
            "Loss at 283: 0.283\n",
            "Loss at 284: 0.282\n",
            "Loss at 285: 0.281\n",
            "Loss at 286: 0.281\n",
            "Loss at 287: 0.280\n",
            "Loss at 288: 0.280\n",
            "Loss at 289: 0.279\n",
            "Loss at 290: 0.279\n",
            "Loss at 291: 0.278\n",
            "Loss at 292: 0.278\n",
            "Loss at 293: 0.277\n",
            "Loss at 294: 0.277\n",
            "Loss at 295: 0.276\n",
            "Loss at 296: 0.276\n",
            "Loss at 297: 0.275\n",
            "Loss at 298: 0.275\n",
            "Loss at 299: 0.274\n",
            "Loss at 300: 0.274\n",
            "Loss at 301: 0.273\n",
            "Loss at 302: 0.273\n",
            "Loss at 303: 0.272\n",
            "Loss at 304: 0.272\n",
            "Loss at 305: 0.271\n",
            "Loss at 306: 0.271\n",
            "Loss at 307: 0.270\n",
            "Loss at 308: 0.270\n",
            "Loss at 309: 0.269\n",
            "Loss at 310: 0.269\n",
            "Loss at 311: 0.268\n",
            "Loss at 312: 0.268\n",
            "Loss at 313: 0.267\n",
            "Loss at 314: 0.267\n",
            "Loss at 315: 0.266\n",
            "Loss at 316: 0.266\n",
            "Loss at 317: 0.265\n",
            "Loss at 318: 0.265\n",
            "Loss at 319: 0.264\n",
            "Loss at 320: 0.264\n",
            "Loss at 321: 0.263\n",
            "Loss at 322: 0.263\n",
            "Loss at 323: 0.262\n",
            "Loss at 324: 0.262\n",
            "Loss at 325: 0.261\n",
            "Loss at 326: 0.261\n",
            "Loss at 327: 0.260\n",
            "Loss at 328: 0.260\n",
            "Loss at 329: 0.259\n",
            "Loss at 330: 0.259\n",
            "Loss at 331: 0.258\n",
            "Loss at 332: 0.258\n",
            "Loss at 333: 0.257\n",
            "Loss at 334: 0.257\n",
            "Loss at 335: 0.257\n",
            "Loss at 336: 0.256\n",
            "Loss at 337: 0.256\n",
            "Loss at 338: 0.255\n",
            "Loss at 339: 0.255\n",
            "Loss at 340: 0.254\n",
            "Loss at 341: 0.254\n",
            "Loss at 342: 0.253\n",
            "Loss at 343: 0.253\n",
            "Loss at 344: 0.252\n",
            "Loss at 345: 0.252\n",
            "Loss at 346: 0.252\n",
            "Loss at 347: 0.251\n",
            "Loss at 348: 0.251\n",
            "Loss at 349: 0.250\n",
            "Loss at 350: 0.250\n",
            "Loss at 351: 0.249\n",
            "Loss at 352: 0.249\n",
            "Loss at 353: 0.248\n",
            "Loss at 354: 0.248\n",
            "Loss at 355: 0.248\n",
            "Loss at 356: 0.247\n",
            "Loss at 357: 0.247\n",
            "Loss at 358: 0.246\n",
            "Loss at 359: 0.246\n",
            "Loss at 360: 0.245\n",
            "Loss at 361: 0.245\n",
            "Loss at 362: 0.245\n",
            "Loss at 363: 0.244\n",
            "Loss at 364: 0.244\n",
            "Loss at 365: 0.243\n",
            "Loss at 366: 0.243\n",
            "Loss at 367: 0.242\n",
            "Loss at 368: 0.242\n",
            "Loss at 369: 0.242\n",
            "Loss at 370: 0.241\n",
            "Loss at 371: 0.241\n",
            "Loss at 372: 0.240\n",
            "Loss at 373: 0.240\n",
            "Loss at 374: 0.239\n",
            "Loss at 375: 0.239\n",
            "Loss at 376: 0.239\n",
            "Loss at 377: 0.238\n",
            "Loss at 378: 0.238\n",
            "Loss at 379: 0.237\n",
            "Loss at 380: 0.237\n",
            "Loss at 381: 0.237\n",
            "Loss at 382: 0.236\n",
            "Loss at 383: 0.236\n",
            "Loss at 384: 0.235\n",
            "Loss at 385: 0.235\n",
            "Loss at 386: 0.235\n",
            "Loss at 387: 0.234\n",
            "Loss at 388: 0.234\n",
            "Loss at 389: 0.233\n",
            "Loss at 390: 0.233\n",
            "Loss at 391: 0.233\n",
            "Loss at 392: 0.232\n",
            "Loss at 393: 0.232\n",
            "Loss at 394: 0.231\n",
            "Loss at 395: 0.231\n",
            "Loss at 396: 0.231\n",
            "Loss at 397: 0.230\n",
            "Loss at 398: 0.230\n",
            "Loss at 399: 0.229\n",
            "Loss at 400: 0.229\n",
            "Loss at 401: 0.229\n",
            "Loss at 402: 0.228\n",
            "Loss at 403: 0.228\n",
            "Loss at 404: 0.227\n",
            "Loss at 405: 0.227\n",
            "Loss at 406: 0.227\n",
            "Loss at 407: 0.226\n",
            "Loss at 408: 0.226\n",
            "Loss at 409: 0.226\n",
            "Loss at 410: 0.225\n",
            "Loss at 411: 0.225\n",
            "Loss at 412: 0.224\n",
            "Loss at 413: 0.224\n",
            "Loss at 414: 0.224\n",
            "Loss at 415: 0.223\n",
            "Loss at 416: 0.223\n",
            "Loss at 417: 0.223\n",
            "Loss at 418: 0.222\n",
            "Loss at 419: 0.222\n",
            "Loss at 420: 0.221\n",
            "Loss at 421: 0.221\n",
            "Loss at 422: 0.221\n",
            "Loss at 423: 0.220\n",
            "Loss at 424: 0.220\n",
            "Loss at 425: 0.220\n",
            "Loss at 426: 0.219\n",
            "Loss at 427: 0.219\n",
            "Loss at 428: 0.218\n",
            "Loss at 429: 0.218\n",
            "Loss at 430: 0.218\n",
            "Loss at 431: 0.217\n",
            "Loss at 432: 0.217\n",
            "Loss at 433: 0.217\n",
            "Loss at 434: 0.216\n",
            "Loss at 435: 0.216\n",
            "Loss at 436: 0.216\n",
            "Loss at 437: 0.215\n",
            "Loss at 438: 0.215\n",
            "Loss at 439: 0.215\n",
            "Loss at 440: 0.214\n",
            "Loss at 441: 0.214\n",
            "Loss at 442: 0.214\n",
            "Loss at 443: 0.213\n",
            "Loss at 444: 0.213\n",
            "Loss at 445: 0.212\n",
            "Loss at 446: 0.212\n",
            "Loss at 447: 0.212\n",
            "Loss at 448: 0.211\n",
            "Loss at 449: 0.211\n",
            "Loss at 450: 0.211\n",
            "Loss at 451: 0.210\n",
            "Loss at 452: 0.210\n",
            "Loss at 453: 0.210\n",
            "Loss at 454: 0.209\n",
            "Loss at 455: 0.209\n",
            "Loss at 456: 0.209\n",
            "Loss at 457: 0.208\n",
            "Loss at 458: 0.208\n",
            "Loss at 459: 0.208\n",
            "Loss at 460: 0.207\n",
            "Loss at 461: 0.207\n",
            "Loss at 462: 0.207\n",
            "Loss at 463: 0.206\n",
            "Loss at 464: 0.206\n",
            "Loss at 465: 0.206\n",
            "Loss at 466: 0.205\n",
            "Loss at 467: 0.205\n",
            "Loss at 468: 0.205\n",
            "Loss at 469: 0.204\n",
            "Loss at 470: 0.204\n",
            "Loss at 471: 0.204\n",
            "Loss at 472: 0.203\n",
            "Loss at 473: 0.203\n",
            "Loss at 474: 0.203\n",
            "Loss at 475: 0.202\n",
            "Loss at 476: 0.202\n",
            "Loss at 477: 0.202\n",
            "Loss at 478: 0.201\n",
            "Loss at 479: 0.201\n",
            "Loss at 480: 0.201\n",
            "Loss at 481: 0.201\n",
            "Loss at 482: 0.200\n",
            "Loss at 483: 0.200\n",
            "Loss at 484: 0.200\n",
            "Loss at 485: 0.199\n",
            "Loss at 486: 0.199\n",
            "Loss at 487: 0.199\n",
            "Loss at 488: 0.198\n",
            "Loss at 489: 0.198\n",
            "Loss at 490: 0.198\n",
            "Loss at 491: 0.197\n",
            "Loss at 492: 0.197\n",
            "Loss at 493: 0.197\n",
            "Loss at 494: 0.196\n",
            "Loss at 495: 0.196\n",
            "Loss at 496: 0.196\n",
            "Loss at 497: 0.196\n",
            "Loss at 498: 0.195\n",
            "Loss at 499: 0.195\n",
            "Loss at 500: 0.195\n",
            "Loss at 501: 0.194\n",
            "Loss at 502: 0.194\n",
            "Loss at 503: 0.194\n",
            "Loss at 504: 0.193\n",
            "Loss at 505: 0.193\n",
            "Loss at 506: 0.193\n",
            "Loss at 507: 0.193\n",
            "Loss at 508: 0.192\n",
            "Loss at 509: 0.192\n",
            "Loss at 510: 0.192\n",
            "Loss at 511: 0.191\n",
            "Loss at 512: 0.191\n",
            "Loss at 513: 0.191\n",
            "Loss at 514: 0.190\n",
            "Loss at 515: 0.190\n",
            "Loss at 516: 0.190\n",
            "Loss at 517: 0.190\n",
            "Loss at 518: 0.189\n",
            "Loss at 519: 0.189\n",
            "Loss at 520: 0.189\n",
            "Loss at 521: 0.188\n",
            "Loss at 522: 0.188\n",
            "Loss at 523: 0.188\n",
            "Loss at 524: 0.188\n",
            "Loss at 525: 0.187\n",
            "Loss at 526: 0.187\n",
            "Loss at 527: 0.187\n",
            "Loss at 528: 0.186\n",
            "Loss at 529: 0.186\n",
            "Loss at 530: 0.186\n",
            "Loss at 531: 0.186\n",
            "Loss at 532: 0.185\n",
            "Loss at 533: 0.185\n",
            "Loss at 534: 0.185\n",
            "Loss at 535: 0.184\n",
            "Loss at 536: 0.184\n",
            "Loss at 537: 0.184\n",
            "Loss at 538: 0.184\n",
            "Loss at 539: 0.183\n",
            "Loss at 540: 0.183\n",
            "Loss at 541: 0.183\n",
            "Loss at 542: 0.183\n",
            "Loss at 543: 0.182\n",
            "Loss at 544: 0.182\n",
            "Loss at 545: 0.182\n",
            "Loss at 546: 0.181\n",
            "Loss at 547: 0.181\n",
            "Loss at 548: 0.181\n",
            "Loss at 549: 0.181\n",
            "Loss at 550: 0.180\n",
            "Loss at 551: 0.180\n",
            "Loss at 552: 0.180\n",
            "Loss at 553: 0.180\n",
            "Loss at 554: 0.179\n",
            "Loss at 555: 0.179\n",
            "Loss at 556: 0.179\n",
            "Loss at 557: 0.178\n",
            "Loss at 558: 0.178\n",
            "Loss at 559: 0.178\n",
            "Loss at 560: 0.178\n",
            "Loss at 561: 0.177\n",
            "Loss at 562: 0.177\n",
            "Loss at 563: 0.177\n",
            "Loss at 564: 0.177\n",
            "Loss at 565: 0.176\n",
            "Loss at 566: 0.176\n",
            "Loss at 567: 0.176\n",
            "Loss at 568: 0.176\n",
            "Loss at 569: 0.175\n",
            "Loss at 570: 0.175\n",
            "Loss at 571: 0.175\n",
            "Loss at 572: 0.175\n",
            "Loss at 573: 0.174\n",
            "Loss at 574: 0.174\n",
            "Loss at 575: 0.174\n",
            "Loss at 576: 0.173\n",
            "Loss at 577: 0.173\n",
            "Loss at 578: 0.173\n",
            "Loss at 579: 0.173\n",
            "Loss at 580: 0.172\n",
            "Loss at 581: 0.172\n",
            "Loss at 582: 0.172\n",
            "Loss at 583: 0.172\n",
            "Loss at 584: 0.171\n",
            "Loss at 585: 0.171\n",
            "Loss at 586: 0.171\n",
            "Loss at 587: 0.171\n",
            "Loss at 588: 0.170\n",
            "Loss at 589: 0.170\n",
            "Loss at 590: 0.170\n",
            "Loss at 591: 0.170\n",
            "Loss at 592: 0.169\n",
            "Loss at 593: 0.169\n",
            "Loss at 594: 0.169\n",
            "Loss at 595: 0.169\n",
            "Loss at 596: 0.168\n",
            "Loss at 597: 0.168\n",
            "Loss at 598: 0.168\n",
            "Loss at 599: 0.168\n",
            "Loss at 600: 0.167\n",
            "Loss at 601: 0.167\n",
            "Loss at 602: 0.167\n",
            "Loss at 603: 0.167\n",
            "Loss at 604: 0.166\n",
            "Loss at 605: 0.166\n",
            "Loss at 606: 0.166\n",
            "Loss at 607: 0.166\n",
            "Loss at 608: 0.165\n",
            "Loss at 609: 0.165\n",
            "Loss at 610: 0.165\n",
            "Loss at 611: 0.165\n",
            "Loss at 612: 0.164\n",
            "Loss at 613: 0.164\n",
            "Loss at 614: 0.164\n",
            "Loss at 615: 0.163\n",
            "Loss at 616: 0.163\n",
            "Loss at 617: 0.163\n",
            "Loss at 618: 0.163\n",
            "Loss at 619: 0.162\n",
            "Loss at 620: 0.162\n",
            "Loss at 621: 0.162\n",
            "Loss at 622: 0.161\n",
            "Loss at 623: 0.161\n",
            "Loss at 624: 0.160\n",
            "Loss at 625: 0.160\n",
            "Loss at 626: 0.159\n",
            "Loss at 627: 0.159\n",
            "Loss at 628: 0.158\n",
            "Loss at 629: 0.157\n",
            "Loss at 630: 0.156\n",
            "Loss at 631: 0.154\n",
            "Loss at 632: 0.150\n",
            "Loss at 633: 0.143\n",
            "Loss at 634: 0.127\n",
            "Loss at 635: 0.100\n",
            "Loss at 636: 0.078\n",
            "Loss at 637: 0.066\n",
            "Loss at 638: 0.058\n",
            "Loss at 639: 0.053\n",
            "Loss at 640: 0.049\n",
            "Loss at 641: 0.046\n",
            "Loss at 642: 0.044\n",
            "Loss at 643: 0.042\n",
            "Loss at 644: 0.040\n",
            "Loss at 645: 0.039\n",
            "Loss at 646: 0.038\n",
            "Loss at 647: 0.037\n",
            "Loss at 648: 0.036\n",
            "Loss at 649: 0.035\n",
            "Loss at 650: 0.035\n",
            "Loss at 651: 0.034\n",
            "Loss at 652: 0.033\n",
            "Loss at 653: 0.033\n",
            "Loss at 654: 0.032\n",
            "Loss at 655: 0.032\n",
            "Loss at 656: 0.032\n",
            "Loss at 657: 0.031\n",
            "Loss at 658: 0.031\n",
            "Loss at 659: 0.030\n",
            "Loss at 660: 0.030\n",
            "Loss at 661: 0.030\n",
            "Loss at 662: 0.030\n",
            "Loss at 663: 0.029\n",
            "Loss at 664: 0.029\n",
            "Loss at 665: 0.029\n",
            "Loss at 666: 0.028\n",
            "Loss at 667: 0.028\n",
            "Loss at 668: 0.028\n",
            "Loss at 669: 0.028\n",
            "Loss at 670: 0.028\n",
            "Loss at 671: 0.027\n",
            "Loss at 672: 0.027\n",
            "Loss at 673: 0.027\n",
            "Loss at 674: 0.027\n",
            "Loss at 675: 0.027\n",
            "Loss at 676: 0.027\n",
            "Loss at 677: 0.026\n",
            "Loss at 678: 0.026\n",
            "Loss at 679: 0.026\n",
            "Loss at 680: 0.026\n",
            "Loss at 681: 0.026\n",
            "Loss at 682: 0.026\n",
            "Loss at 683: 0.026\n",
            "Loss at 684: 0.025\n",
            "Loss at 685: 0.025\n",
            "Loss at 686: 0.025\n",
            "Loss at 687: 0.025\n",
            "Loss at 688: 0.025\n",
            "Loss at 689: 0.025\n",
            "Loss at 690: 0.025\n",
            "Loss at 691: 0.025\n",
            "Loss at 692: 0.025\n",
            "Loss at 693: 0.024\n",
            "Loss at 694: 0.024\n",
            "Loss at 695: 0.024\n",
            "Loss at 696: 0.024\n",
            "Loss at 697: 0.024\n",
            "Loss at 698: 0.024\n",
            "Loss at 699: 0.024\n",
            "Loss at 700: 0.024\n",
            "Loss at 701: 0.024\n",
            "Loss at 702: 0.024\n",
            "Loss at 703: 0.023\n",
            "Loss at 704: 0.023\n",
            "Loss at 705: 0.023\n",
            "Loss at 706: 0.023\n",
            "Loss at 707: 0.023\n",
            "Loss at 708: 0.023\n",
            "Loss at 709: 0.023\n",
            "Loss at 710: 0.023\n",
            "Loss at 711: 0.023\n",
            "Loss at 712: 0.023\n",
            "Loss at 713: 0.023\n",
            "Loss at 714: 0.023\n",
            "Loss at 715: 0.023\n",
            "Loss at 716: 0.022\n",
            "Loss at 717: 0.022\n",
            "Loss at 718: 0.022\n",
            "Loss at 719: 0.022\n",
            "Loss at 720: 0.022\n",
            "Loss at 721: 0.022\n",
            "Loss at 722: 0.022\n",
            "Loss at 723: 0.022\n",
            "Loss at 724: 0.022\n",
            "Loss at 725: 0.022\n",
            "Loss at 726: 0.022\n",
            "Loss at 727: 0.022\n",
            "Loss at 728: 0.022\n",
            "Loss at 729: 0.022\n",
            "Loss at 730: 0.022\n",
            "Loss at 731: 0.021\n",
            "Loss at 732: 0.021\n",
            "Loss at 733: 0.021\n",
            "Loss at 734: 0.021\n",
            "Loss at 735: 0.021\n",
            "Loss at 736: 0.021\n",
            "Loss at 737: 0.021\n",
            "Loss at 738: 0.021\n",
            "Loss at 739: 0.021\n",
            "Loss at 740: 0.021\n",
            "Loss at 741: 0.021\n",
            "Loss at 742: 0.021\n",
            "Loss at 743: 0.021\n",
            "Loss at 744: 0.021\n",
            "Loss at 745: 0.021\n",
            "Loss at 746: 0.021\n",
            "Loss at 747: 0.021\n",
            "Loss at 748: 0.021\n",
            "Loss at 749: 0.020\n",
            "Loss at 750: 0.020\n",
            "Loss at 751: 0.020\n",
            "Loss at 752: 0.020\n",
            "Loss at 753: 0.020\n",
            "Loss at 754: 0.020\n",
            "Loss at 755: 0.020\n",
            "Loss at 756: 0.020\n",
            "Loss at 757: 0.020\n",
            "Loss at 758: 0.020\n",
            "Loss at 759: 0.020\n",
            "Loss at 760: 0.020\n",
            "Loss at 761: 0.020\n",
            "Loss at 762: 0.020\n",
            "Loss at 763: 0.020\n",
            "Loss at 764: 0.020\n",
            "Loss at 765: 0.020\n",
            "Loss at 766: 0.020\n",
            "Loss at 767: 0.020\n",
            "Loss at 768: 0.020\n",
            "Loss at 769: 0.020\n",
            "Loss at 770: 0.020\n",
            "Loss at 771: 0.019\n",
            "Loss at 772: 0.019\n",
            "Loss at 773: 0.019\n",
            "Loss at 774: 0.019\n",
            "Loss at 775: 0.019\n",
            "Loss at 776: 0.019\n",
            "Loss at 777: 0.019\n",
            "Loss at 778: 0.019\n",
            "Loss at 779: 0.019\n",
            "Loss at 780: 0.019\n",
            "Loss at 781: 0.019\n",
            "Loss at 782: 0.019\n",
            "Loss at 783: 0.019\n",
            "Loss at 784: 0.019\n",
            "Loss at 785: 0.019\n",
            "Loss at 786: 0.019\n",
            "Loss at 787: 0.019\n",
            "Loss at 788: 0.019\n",
            "Loss at 789: 0.019\n",
            "Loss at 790: 0.019\n",
            "Loss at 791: 0.019\n",
            "Loss at 792: 0.019\n",
            "Loss at 793: 0.019\n",
            "Loss at 794: 0.019\n",
            "Loss at 795: 0.019\n",
            "Loss at 796: 0.019\n",
            "Loss at 797: 0.018\n",
            "Loss at 798: 0.018\n",
            "Loss at 799: 0.018\n",
            "Loss at 800: 0.018\n",
            "Loss at 801: 0.018\n",
            "Loss at 802: 0.018\n",
            "Loss at 803: 0.018\n",
            "Loss at 804: 0.018\n",
            "Loss at 805: 0.018\n",
            "Loss at 806: 0.018\n",
            "Loss at 807: 0.018\n",
            "Loss at 808: 0.018\n",
            "Loss at 809: 0.018\n",
            "Loss at 810: 0.018\n",
            "Loss at 811: 0.018\n",
            "Loss at 812: 0.018\n",
            "Loss at 813: 0.018\n",
            "Loss at 814: 0.018\n",
            "Loss at 815: 0.018\n",
            "Loss at 816: 0.018\n",
            "Loss at 817: 0.018\n",
            "Loss at 818: 0.018\n",
            "Loss at 819: 0.018\n",
            "Loss at 820: 0.018\n",
            "Loss at 821: 0.018\n",
            "Loss at 822: 0.018\n",
            "Loss at 823: 0.018\n",
            "Loss at 824: 0.018\n",
            "Loss at 825: 0.018\n",
            "Loss at 826: 0.018\n",
            "Loss at 827: 0.018\n",
            "Loss at 828: 0.018\n",
            "Loss at 829: 0.017\n",
            "Loss at 830: 0.017\n",
            "Loss at 831: 0.017\n",
            "Loss at 832: 0.017\n",
            "Loss at 833: 0.017\n",
            "Loss at 834: 0.017\n",
            "Loss at 835: 0.017\n",
            "Loss at 836: 0.017\n",
            "Loss at 837: 0.017\n",
            "Loss at 838: 0.017\n",
            "Loss at 839: 0.017\n",
            "Loss at 840: 0.017\n",
            "Loss at 841: 0.017\n",
            "Loss at 842: 0.017\n",
            "Loss at 843: 0.017\n",
            "Loss at 844: 0.017\n",
            "Loss at 845: 0.017\n",
            "Loss at 846: 0.017\n",
            "Loss at 847: 0.017\n",
            "Loss at 848: 0.017\n",
            "Loss at 849: 0.017\n",
            "Loss at 850: 0.017\n",
            "Loss at 851: 0.017\n",
            "Loss at 852: 0.017\n",
            "Loss at 853: 0.017\n",
            "Loss at 854: 0.017\n",
            "Loss at 855: 0.017\n",
            "Loss at 856: 0.017\n",
            "Loss at 857: 0.017\n",
            "Loss at 858: 0.017\n",
            "Loss at 859: 0.017\n",
            "Loss at 860: 0.017\n",
            "Loss at 861: 0.017\n",
            "Loss at 862: 0.017\n",
            "Loss at 863: 0.017\n",
            "Loss at 864: 0.017\n",
            "Loss at 865: 0.017\n",
            "Loss at 866: 0.017\n",
            "Loss at 867: 0.016\n",
            "Loss at 868: 0.016\n",
            "Loss at 869: 0.016\n",
            "Loss at 870: 0.016\n",
            "Loss at 871: 0.016\n",
            "Loss at 872: 0.016\n",
            "Loss at 873: 0.016\n",
            "Loss at 874: 0.016\n",
            "Loss at 875: 0.016\n",
            "Loss at 876: 0.016\n",
            "Loss at 877: 0.016\n",
            "Loss at 878: 0.016\n",
            "Loss at 879: 0.016\n",
            "Loss at 880: 0.016\n",
            "Loss at 881: 0.016\n",
            "Loss at 882: 0.016\n",
            "Loss at 883: 0.016\n",
            "Loss at 884: 0.016\n",
            "Loss at 885: 0.016\n",
            "Loss at 886: 0.016\n",
            "Loss at 887: 0.016\n",
            "Loss at 888: 0.016\n",
            "Loss at 889: 0.016\n",
            "Loss at 890: 0.016\n",
            "Loss at 891: 0.016\n",
            "Loss at 892: 0.016\n",
            "Loss at 893: 0.016\n",
            "Loss at 894: 0.016\n",
            "Loss at 895: 0.016\n",
            "Loss at 896: 0.016\n",
            "Loss at 897: 0.016\n",
            "Loss at 898: 0.016\n",
            "Loss at 899: 0.016\n",
            "Loss at 900: 0.016\n",
            "Loss at 901: 0.016\n",
            "Loss at 902: 0.016\n",
            "Loss at 903: 0.016\n",
            "Loss at 904: 0.016\n",
            "Loss at 905: 0.016\n",
            "Loss at 906: 0.016\n",
            "Loss at 907: 0.016\n",
            "Loss at 908: 0.016\n",
            "Loss at 909: 0.016\n",
            "Loss at 910: 0.016\n",
            "Loss at 911: 0.015\n",
            "Loss at 912: 0.015\n",
            "Loss at 913: 0.015\n",
            "Loss at 914: 0.015\n",
            "Loss at 915: 0.015\n",
            "Loss at 916: 0.015\n",
            "Loss at 917: 0.015\n",
            "Loss at 918: 0.015\n",
            "Loss at 919: 0.015\n",
            "Loss at 920: 0.015\n",
            "Loss at 921: 0.015\n",
            "Loss at 922: 0.015\n",
            "Loss at 923: 0.015\n",
            "Loss at 924: 0.015\n",
            "Loss at 925: 0.015\n",
            "Loss at 926: 0.015\n",
            "Loss at 927: 0.015\n",
            "Loss at 928: 0.015\n",
            "Loss at 929: 0.015\n",
            "Loss at 930: 0.015\n",
            "Loss at 931: 0.015\n",
            "Loss at 932: 0.015\n",
            "Loss at 933: 0.015\n",
            "Loss at 934: 0.015\n",
            "Loss at 935: 0.015\n",
            "Loss at 936: 0.015\n",
            "Loss at 937: 0.015\n",
            "Loss at 938: 0.015\n",
            "Loss at 939: 0.015\n",
            "Loss at 940: 0.015\n",
            "Loss at 941: 0.015\n",
            "Loss at 942: 0.015\n",
            "Loss at 943: 0.015\n",
            "Loss at 944: 0.015\n",
            "Loss at 945: 0.015\n",
            "Loss at 946: 0.015\n",
            "Loss at 947: 0.015\n",
            "Loss at 948: 0.015\n",
            "Loss at 949: 0.015\n",
            "Loss at 950: 0.015\n",
            "Loss at 951: 0.015\n",
            "Loss at 952: 0.015\n",
            "Loss at 953: 0.015\n",
            "Loss at 954: 0.015\n",
            "Loss at 955: 0.015\n",
            "Loss at 956: 0.015\n",
            "Loss at 957: 0.015\n",
            "Loss at 958: 0.015\n",
            "Loss at 959: 0.015\n",
            "Loss at 960: 0.015\n",
            "Loss at 961: 0.015\n",
            "Loss at 962: 0.014\n",
            "Loss at 963: 0.014\n",
            "Loss at 964: 0.014\n",
            "Loss at 965: 0.014\n",
            "Loss at 966: 0.014\n",
            "Loss at 967: 0.014\n",
            "Loss at 968: 0.014\n",
            "Loss at 969: 0.014\n",
            "Loss at 970: 0.014\n",
            "Loss at 971: 0.014\n",
            "Loss at 972: 0.014\n",
            "Loss at 973: 0.014\n",
            "Loss at 974: 0.014\n",
            "Loss at 975: 0.014\n",
            "Loss at 976: 0.014\n",
            "Loss at 977: 0.014\n",
            "Loss at 978: 0.014\n",
            "Loss at 979: 0.014\n",
            "Loss at 980: 0.014\n",
            "Loss at 981: 0.014\n",
            "Loss at 982: 0.014\n",
            "Loss at 983: 0.014\n",
            "Loss at 984: 0.014\n",
            "Loss at 985: 0.014\n",
            "Loss at 986: 0.014\n",
            "Loss at 987: 0.014\n",
            "Loss at 988: 0.014\n",
            "Loss at 989: 0.014\n",
            "Loss at 990: 0.014\n",
            "Loss at 991: 0.014\n",
            "Loss at 992: 0.014\n",
            "Loss at 993: 0.014\n",
            "Loss at 994: 0.014\n",
            "Loss at 995: 0.014\n",
            "Loss at 996: 0.014\n",
            "Loss at 997: 0.014\n",
            "Loss at 998: 0.014\n",
            "Loss at 999: 0.014\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the `MLP` with the following structure:\n",
        "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
        "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "model = MLP([Linear(784,50), Sigmoid(), Linear(50,50), Sigmoid(), Linear(50,3), SoftMaxLayer()], CELoss(), GradientDescent(0.01))\n",
        "#############################\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "# Train the network using only `x_train` and `y_train` (no validation)\n",
        "train(model, epochs, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJec2xRJmY37"
      },
      "source": [
        "Let's plot the loss value for each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ymaQNn70cdnZ",
        "outputId": "f24199b7-8555-4863-a555-fd752f36db70"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(model.losses)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz3yqRa1cdna"
      },
      "source": [
        "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRqwXho7cdnd",
        "outputId": "d2227190-90a4-44d3-f262-e32a66d6a326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9922685272487272\n"
          ]
        }
      ],
      "source": [
        "# Compute the accuracy on the testing set\n",
        "#############################\n",
        "# Your code goes here (7 points)\n",
        "predicted = model.predict(x_test)\n",
        "correctCount = 0\n",
        "for label in range(len(predicted)):\n",
        "  if predicted[label] == y_test[label]:\n",
        "    correctCount+=1\n",
        "acc = correctCount/len(y_test)\n",
        "#############################\n",
        "\n",
        "print(acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
